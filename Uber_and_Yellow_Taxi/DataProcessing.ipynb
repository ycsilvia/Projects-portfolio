{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install needed libraries\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install geopandas\n",
    "!pip install pytest\n",
    "!pip install keplergl\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import keplergl\n",
    "from keplergl import KeplerGl\n",
    "import statistics\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other constants\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distance\n",
    "_**TODO:** Since we do not have trip distance information in our data, we need to calculate the distance from the latitude and longitude information of the pickup and dropoff locations. Here we use functions to complete the calculation and add the resulting trip distances to the dataframe._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the two coordinates\n",
    "def calculate_distance(from_coord: list, to_coord: list) -> float:\n",
    "    R = 6373.0\n",
    "    lat1 = math.radians(from_coord[0])\n",
    "    lon1 = math.radians(from_coord[1])\n",
    "    lat2 = math.radians(to_coord[0])\n",
    "    lon2 = math.radians(to_coord[1])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the dataset that is not given the trip distance, calculate the distance using the given coordinate data and add it to the dataframe\n",
    "def add_distance_column(dataframe: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    distance = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        distance.append(calculate_distance((row['pickup_latitude'], row['pickup_longitude']), (row['dropoff_latitude'], row['dropoff_longitude'])))\n",
    "    dataframe['trip_distance'] = distance\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Uber Data\n",
    "\n",
    "_**TODO:** Read uber's trip data and process it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load uber data and clean the data\n",
    "def load_and_clean_uber_data(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    df = pd.read_csv(csv_file, on_bad_lines='skip')\n",
    "    df.columns = df.columns.str.lower()\n",
    "    add_distance_column(df)\n",
    "    df.drop(df.columns.difference(['pickup_datetime',\n",
    "                                     'trip_distance', \n",
    "                                     'pickup_latitude', \n",
    "                                     'pickup_longitude', \n",
    "                                     'dropoff_latitude', \n",
    "                                     'dropoff_longitude']), 1, inplace=True)\n",
    "\n",
    "    # remove rows start and/or end outside of the following latitude/longitude coordinate box: \n",
    "    # (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    df=df[df[\"pickup_longitude\"] <= -73.717047]  \n",
    "    df=df[df[\"pickup_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"pickup_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"pickup_latitude\"] <= 40.908524]\n",
    "    df=df[df[\"dropoff_longitude\"] <= -73.717047]\n",
    "    df=df[df[\"dropoff_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"dropoff_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"dropoff_latitude\"] <= 40.908524]\n",
    "\n",
    "    # remove invalid rows thtat pickup time is 0\n",
    "    df = df.loc[df[\"pickup_datetime\"] != 0.0]\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\1263730049.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    }
   ],
   "source": [
    "uber_data = load_and_clean_uber_data(UBER_CSV)\n",
    "uber_data.to_csv(\"uber_data\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Taxi Data\n",
    "\n",
    "_**TODO:** Use Beautiful Soup to get the required taxi data links from the New York government data website (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), read the data and convert the data into the same format._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls() -> list:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = soup.find_all(lambda tag:'title' in tag.attrs and tag.attrs['title'] == \"Yellow Taxi Trip Records\")\n",
    "    hrefs = [link.get('href') for link in links]\n",
    "    # Filter the links based on the desired years (2009 to 2015)\n",
    "    hrefs_filtered = [href for href in hrefs \n",
    "                  if any(year in href for year in map(str, range(2009, 2015)))\n",
    "                  or (any(f\"2015-{month:02}\" in href for month in range(1, 7)))]\n",
    "    return hrefs_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that converts location to coordinates, and generate a dataframe\n",
    "def convert_id_to_coord(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    shapefile = gpd.read_file(r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\taxi_zones\\taxi_zones.shp\")\n",
    "    # Convert the geometry column in the shapefile into specific coordinates of latitude and longitude\n",
    "    shapefile = shapefile.to_crs(4326)\n",
    "    shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
    "    shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
    "    \n",
    "    df = df\n",
    "    df = df.loc[df[\"pulocationid\"] <= 263]\n",
    "    df = df.loc[df[\"pulocationid\"] != 0]\n",
    "    df = df.loc[df[\"dolocationid\"] <= 263]\n",
    "    df = df.loc[df[\"dolocationid\"] != 0]\n",
    "    # convert location IDs into longitude and latitude\n",
    "    PUlongitude = []\n",
    "    PUlatitude = []\n",
    "    DOlongitude = []\n",
    "    DOlatitude = []\n",
    "    # convert the pickup location IDs into longitude and latitude\n",
    "    for i in df['pulocationid']:\n",
    "        PUlatitude.append(shapefile['latitude'][i-1])\n",
    "        PUlongitude.append(shapefile['longitude'][i-1])\n",
    "    for i in df['dolocationid']:\n",
    "        DOlatitude.append(shapefile['latitude'][i-1])\n",
    "        DOlongitude.append(shapefile['longitude'][i-1])\n",
    "        \n",
    "    df['pickup_longitude'] = PUlongitude\n",
    "    df['pickup_latitude'] = PUlatitude\n",
    "    df['dropoff_longitude'] = DOlongitude\n",
    "    df['dropoff_latitude'] = DOlatitude\n",
    "    # convert the drop off location IDs into longitude and latitude\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain taxi data and clean the data\n",
    "def get_and_clean_month_taxi_data(url: str) -> pd.core.frame.DataFrame:\n",
    "\n",
    "    df = pd.read_parquet(url)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df_taxi = pd.DataFrame()\n",
    "\n",
    "    # keep necessary columns into a new dataframe\n",
    "    if 'tpep_pickup_datetime' in df.columns:\n",
    "        df=df.rename(columns = {'tpep_pickup_datetime':'pickup_datetime',\n",
    "                                'tip_amount' : 'tip_amount'})\n",
    "        df=convert_id_to_coord(df)\n",
    "        \n",
    "    elif 'trip_pickup_datetime' in df.columns:\n",
    "        df=df.rename(columns = {'trip_pickup_datetime':'pickup_datetime', \n",
    "                                'start_lon': 'pickup_longitude',\n",
    "                                'start_lat': 'pickup_latitude',\n",
    "                                'end_lon': 'dropoff_longitude',\n",
    "                                'end_lat': 'dropoff_latitude',\n",
    "                                'tip_amt' : 'tip_amount'})\n",
    "        \n",
    "    df.drop(df.columns.difference(['pickup_datetime',\n",
    "                                    'trip_distance', \n",
    "                                    'pickup_latitude', \n",
    "                                    'pickup_longitude', \n",
    "                                    'dropoff_latitude', \n",
    "                                    'dropoff_longitude',\n",
    "                                   'tip_amount']), 1, inplace=True)\n",
    "    \n",
    "    df=df[df[\"pickup_longitude\"] <= -73.717047]  \n",
    "    df=df[df[\"pickup_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"pickup_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"pickup_latitude\"] <= 40.908524]\n",
    "    df=df[df[\"dropoff_longitude\"] <= -73.717047]\n",
    "    df=df[df[\"dropoff_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"dropoff_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"dropoff_latitude\"] <= 40.908524]\n",
    "\n",
    "    df = df.loc[df[\"pickup_datetime\"] != 0.0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Taxi Data\n",
    "\n",
    "_**TODO:** Sampling the taxi data according to the number of uber trips each month. Because the number of yellow taxi data is so large (over 67 million) that it is much larger than the uber data for each time period. Therefore, a random sampling method was used to select a number of yellow cab data equal to the number of uber data in each month to facilitate the analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of uber data per month\n",
    "def get_number_to_sample() -> int:\n",
    "    uber = load_and_clean_uber_data(r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\uber_rides_sample.csv\")\n",
    "    uber.index = pd.to_datetime(uber['pickup_datetime'])\n",
    "    number_each_month = uber.groupby(by=[uber.index.year, uber.index.month]).size()\n",
    "    number = []\n",
    "    for i in range(2009,2015):\n",
    "        for j in range(1,13):\n",
    "            number.append(number_each_month[i][j])\n",
    "    for i in range(1,7):\n",
    "        number.append(number_each_month[2015][i])\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_taxi_data() -> pd.core.frame.DataFrame:\n",
    "    sample_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    number = get_number_to_sample()\n",
    "    # do the sampling of taxi data according to the number of uber trip each month\n",
    "    for i in range(0, 78): \n",
    "        n = number[i]\n",
    "        url = all_parquet_urls[i]\n",
    "        df = get_and_clean_month_taxi_data(url)\n",
    "        df_sample = df.sample(n)\n",
    "        sample_taxi_dataframes.append(df_sample)\n",
    "\n",
    "    taxi_data = pd.concat(sample_taxi_dataframes)\n",
    "    taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    taxi_data = taxi_data.reset_index(drop = True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_sample_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.to_csv(\"taxi_sample\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Weather Data\n",
    "\n",
    "_**TODO:** Read and process weather data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    # read file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    #drop unnecessary colums\n",
    "    df.drop(df.columns.difference(['DATE',\n",
    "                                   'HourlyPrecipitation', \n",
    "                                   'HourlyWindSpeed']), 1, inplace=True)\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.0)\n",
    "    # drop na values\n",
    "    df.dropna(subset=['HourlyWindSpeed'], inplace=True)\n",
    "    # convert \"DATE\" to datetime type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    # convert \"HourlyPrecipitation\" to float type\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    # fill in missing values\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    # cast \"df\" to specified type\n",
    "    df = df.astype({'HourlyWindSpeed': 'float32', 'HourlyPrecipitation': 'float32'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    # read file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Replace data of the string type\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.0)\n",
    "    # convert \"DATE\" to datetime type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    # convert \"HourlyPrecipitation\" to numeric type\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    # convert value of 'na' into 0.0\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    #drop unnecessary colums\n",
    "    df.drop(df.columns.difference(['DATE',\n",
    "                                   'HourlyPrecipitation', \n",
    "                                   'HourlyWindSpeed']), 1, inplace=True)\n",
    "    # calculate hourly average as a daily values\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df = df.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': np.mean, 'HourlyPrecipitation': np.mean})\n",
    "    df['HourlyWindSpeed'] = df['HourlyWindSpeed'].map(lambda x: round(x, 2))\n",
    "    # remame columns\n",
    "    df.rename(columns={'HourlyWindSpeed': 'DailyAverageWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    df = df.astype({'DailyAverageWindSpeed':'float32', 'DailyPrecipitation':'float32', 'DATE' : 'datetime64[ns]'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sunset_sunrise_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.astype({'Sunrise': 'int32', 'Sunset': 'int32', 'DATE':'datetime64[ns]' })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> pd.core.frame.DataFrame:\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = [\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2009_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2010_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2011_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2012_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2013_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2014_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2015_weather.csv\"\n",
    "        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_sunrise_sunset_data() -> pd.core.frame.DataFrame:\n",
    "    sunrise_sunset_dataframes =[]\n",
    "    \n",
    "    weather_csv_files = [\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2009_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2010_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2011_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2012_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2013_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2014_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2015_weather.csv\"\n",
    "        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        sunrise_sunset_dataframe = clean_sunset_sunrise_daily(csv_file)\n",
    "        sunrise_sunset_dataframes.append(sunrise_sunset_dataframe)\n",
    "        \n",
    "    sunrise_sunset_data = pd.concat(sunrise_sunset_dataframes)\n",
    "    sunrise_sunset_data['DATE'] = pd.to_datetime(sunrise_sunset_data['DATE'])\n",
    "    sunrise_sunset_data = sunrise_sunset_data.astype({'Sunrise': 'int32', 'Sunset': 'int32'})\n",
    "    \n",
    "    return sunrise_sunset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()\n",
    "sunrise_sunset_data = load_and_clean_sunrise_sunset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11379</th>\n",
       "      <td>2015-12-31 18:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11380</th>\n",
       "      <td>2015-12-31 19:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11381</th>\n",
       "      <td>2015-12-31 20:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11383</th>\n",
       "      <td>2015-12-31 22:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>2015-12-31 23:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73713 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DATE  HourlyPrecipitation  HourlyWindSpeed\n",
       "0     2009-01-01 00:51:00                  0.0             18.0\n",
       "1     2009-01-01 01:51:00                  0.0             18.0\n",
       "2     2009-01-01 02:51:00                  0.0             18.0\n",
       "3     2009-01-01 03:51:00                  0.0              8.0\n",
       "4     2009-01-01 04:51:00                  0.0             11.0\n",
       "...                   ...                  ...              ...\n",
       "11379 2015-12-31 18:51:00                  0.0              3.0\n",
       "11380 2015-12-31 19:51:00                  0.0              6.0\n",
       "11381 2015-12-31 20:51:00                  0.0             10.0\n",
       "11383 2015-12-31 22:51:00                  0.0              7.0\n",
       "11384 2015-12-31 23:51:00                  0.0              5.0\n",
       "\n",
       "[73713 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.to_csv(\"hourly_weather_data\") \n",
    "daily_weather_data.to_csv(\"daily_weather_data\") \n",
    "sunrise_sunset_data.to_csv(\"sunrise_sunset_data\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
