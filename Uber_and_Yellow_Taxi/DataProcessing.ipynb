{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install needed libraries\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install geopandas\n",
    "!pip install pytest\n",
    "!pip install keplergl\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "import keplergl\n",
    "from keplergl import KeplerGl\n",
    "import statistics\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other constants\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distance\n",
    "_**TODO:** Since we do not have trip distance information in our data, we need to calculate the distance from the latitude and longitude information of the pickup and dropoff locations. Here we use functions to complete the calculation and add the resulting trip distances to the dataframe._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the two coordinates\n",
    "def calculate_distance(from_coord: list, to_coord: list) -> float:\n",
    "    R = 6373.0\n",
    "    lat1 = math.radians(from_coord[0])\n",
    "    lon1 = math.radians(from_coord[1])\n",
    "    lat2 = math.radians(to_coord[0])\n",
    "    lon2 = math.radians(to_coord[1])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the dataset that is not given the trip distance, calculate the distance using the given coordinate data and add it to the dataframe\n",
    "def add_distance_column(dataframe: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    distance = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        distance.append(calculate_distance((row['pickup_latitude'], row['pickup_longitude']), (row['dropoff_latitude'], row['dropoff_longitude'])))\n",
    "    dataframe['trip_distance'] = distance\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Taxi Data\n",
    "\n",
    "_**TODO:** Use Beautiful Soup to get the required taxi data links from the New York government data website (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), read the data and convert the data into the same format._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls() -> list:\n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = soup.find_all(lambda tag:'title' in tag.attrs and tag.attrs['title'] == \"Yellow Taxi Trip Records\")\n",
    "    hrefs = [link.get('href') for link in links]\n",
    "    # Filter the links based on the desired years (2009 to 2015)\n",
    "    hrefs_filtered = [href for href in hrefs \n",
    "                  if any(year in href for year in map(str, range(2009, 2015)))\n",
    "                  or (any(f\"2015-{month:02}\" in href for month in range(1, 7)))]\n",
    "    return hrefs_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that converts location to coordinates, and generate a dataframe\n",
    "def convert_id_to_coord(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    shapefile = gpd.read_file(r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\taxi_zones\\taxi_zones.shp\")\n",
    "    # Convert the geometry column in the shapefile into specific coordinates of latitude and longitude\n",
    "    shapefile = shapefile.to_crs(4326)\n",
    "    shapefile['latitude'] = shapefile['geometry'].centroid.y\n",
    "    shapefile['longitude'] = shapefile['geometry'].centroid.x\n",
    "    \n",
    "    df = df\n",
    "    df = df.loc[df[\"pulocationid\"] <= 263]\n",
    "    df = df.loc[df[\"pulocationid\"] != 0]\n",
    "    df = df.loc[df[\"dolocationid\"] <= 263]\n",
    "    df = df.loc[df[\"dolocationid\"] != 0]\n",
    "    # convert location IDs into longitude and latitude\n",
    "    PUlongitude = []\n",
    "    PUlatitude = []\n",
    "    DOlongitude = []\n",
    "    DOlatitude = []\n",
    "    # convert the pickup location IDs into longitude and latitude\n",
    "    for i in df['pulocationid']:\n",
    "        PUlatitude.append(shapefile['latitude'][i-1])\n",
    "        PUlongitude.append(shapefile['longitude'][i-1])\n",
    "    for i in df['dolocationid']:\n",
    "        DOlatitude.append(shapefile['latitude'][i-1])\n",
    "        DOlongitude.append(shapefile['longitude'][i-1])\n",
    "        \n",
    "    df['pickup_longitude'] = PUlongitude\n",
    "    df['pickup_latitude'] = PUlatitude\n",
    "    df['dropoff_longitude'] = DOlongitude\n",
    "    df['dropoff_latitude'] = DOlatitude\n",
    "    # convert the drop off location IDs into longitude and latitude\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain taxi data and clean the data\n",
    "def get_and_clean_month_taxi_data(url: str) -> pd.core.frame.DataFrame:\n",
    "\n",
    "    df = pd.read_parquet(url)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df_taxi = pd.DataFrame()\n",
    "\n",
    "    # keep necessary columns into a new dataframe\n",
    "    if 'tpep_pickup_datetime' in df.columns:\n",
    "        df=df.rename(columns = {'tpep_pickup_datetime':'pickup_datetime',\n",
    "                                'tip_amount' : 'tip_amount'})\n",
    "        df=convert_id_to_coord(df)\n",
    "        \n",
    "    elif 'trip_pickup_datetime' in df.columns:\n",
    "        df=df.rename(columns = {'trip_pickup_datetime':'pickup_datetime', \n",
    "                                'start_lon': 'pickup_longitude',\n",
    "                                'start_lat': 'pickup_latitude',\n",
    "                                'end_lon': 'dropoff_longitude',\n",
    "                                'end_lat': 'dropoff_latitude',\n",
    "                                'tip_amt' : 'tip_amount'})\n",
    "        \n",
    "    df.drop(df.columns.difference(['pickup_datetime',\n",
    "                                    'trip_distance', \n",
    "                                    'pickup_latitude', \n",
    "                                    'pickup_longitude', \n",
    "                                    'dropoff_latitude', \n",
    "                                    'dropoff_longitude',\n",
    "                                   'tip_amount']), 1, inplace=True)\n",
    "    \n",
    "    df=df[df[\"pickup_longitude\"] <= -73.717047]  \n",
    "    df=df[df[\"pickup_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"pickup_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"pickup_latitude\"] <= 40.908524]\n",
    "    df=df[df[\"dropoff_longitude\"] <= -73.717047]\n",
    "    df=df[df[\"dropoff_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"dropoff_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"dropoff_latitude\"] <= 40.908524]\n",
    "\n",
    "    df = df.loc[df[\"pickup_datetime\"] != 0.0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Uber Data\n",
    "\n",
    "_**TODO:** Read uber's trip data and process it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load uber data and clean the data\n",
    "def load_and_clean_uber_data(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    df = pd.read_csv(csv_file, on_bad_lines='skip')\n",
    "    df.columns = df.columns.str.lower()\n",
    "    add_distance_column(df)\n",
    "    df.drop(df.columns.difference(['pickup_datetime',\n",
    "                                     'trip_distance', \n",
    "                                     'pickup_latitude', \n",
    "                                     'pickup_longitude', \n",
    "                                     'dropoff_latitude', \n",
    "                                     'dropoff_longitude']), 1, inplace=True)\n",
    "\n",
    "    # remove rows start and/or end outside of the following latitude/longitude coordinate box: \n",
    "    # (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    df=df[df[\"pickup_longitude\"] <= -73.717047]  \n",
    "    df=df[df[\"pickup_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"pickup_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"pickup_latitude\"] <= 40.908524]\n",
    "    df=df[df[\"dropoff_longitude\"] <= -73.717047]\n",
    "    df=df[df[\"dropoff_longitude\"] >= -74.242330]\n",
    "    df=df[df[\"dropoff_latitude\"] >= 40.560445]\n",
    "    df=df[df[\"dropoff_latitude\"] <= 40.908524]\n",
    "\n",
    "    # remove invalid rows thtat pickup time is 0\n",
    "    df = df.loc[df[\"pickup_datetime\"] != 0.0]\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Taxi Data\n",
    "\n",
    "_**TODO:** Sampling the taxi data according to the number of uber trips each month. Because the number of yellow taxi data is so large (over 67 million) that it is much larger than the uber data for each time period. Therefore, a random sampling method was used to select a number of yellow cab data equal to the number of uber data in each month to facilitate the analysis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of uber data per month\n",
    "def get_number_to_sample() -> int:\n",
    "    uber = load_and_clean_uber_data(r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\uber_rides_sample.csv\")\n",
    "    uber.index = pd.to_datetime(uber['pickup_datetime'])\n",
    "    number_each_month = uber.groupby(by=[uber.index.year, uber.index.month]).size()\n",
    "    number = []\n",
    "    for i in range(2009,2015):\n",
    "        for j in range(1,13):\n",
    "            number.append(number_each_month[i][j])\n",
    "    for i in range(1,7):\n",
    "        number.append(number_each_month[2015][i])\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\1263730049.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    }
   ],
   "source": [
    "number = get_number_to_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\1263730049.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['pickup_datetime',\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\DataProcessing.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m number \u001b[39m=\u001b[39m get_number_to_sample()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sample_taxi_data\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mcore\u001b[39m.\u001b[39mframe\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     sample_taxi_dataframes \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32mc:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\DataProcessing.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_number_to_sample\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     uber \u001b[39m=\u001b[39m load_and_clean_uber_data(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSilvia\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGitHub\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m4501FinalProject_Group14\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39muber_rides_sample.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     uber\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(uber[\u001b[39m'\u001b[39m\u001b[39mpickup_datetime\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     number_each_month \u001b[39m=\u001b[39m uber\u001b[39m.\u001b[39mgroupby(by\u001b[39m=\u001b[39m[uber\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39myear, uber\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mmonth])\u001b[39m.\u001b[39msize()\n",
      "\u001b[1;32mc:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\DataProcessing.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# remove invalid rows thtat pickup time is 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[df[\u001b[39m\"\u001b[39m\u001b[39mpickup_datetime\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0.0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mpickup_datetime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mpickup_datetime\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Silvia/Documents/GitHub/Projects-portfolio/Uber_and_Yellow_Taxi/DataProcessing.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1068\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1067\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1068\u001b[0m         values \u001b[39m=\u001b[39m convert_listlike(arg\u001b[39m.\u001b[39;49m_values, \u001b[39mformat\u001b[39;49m)\n\u001b[0;32m   1069\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39m_constructor(values, index\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mindex, name\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1070\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[39m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:438\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m infer_datetime_format\n\u001b[0;32m    437\u001b[0m utc \u001b[39m=\u001b[39m tz \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutc\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 438\u001b[0m result, tz_parsed \u001b[39m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    439\u001b[0m     arg,\n\u001b[0;32m    440\u001b[0m     dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[0;32m    441\u001b[0m     yearfirst\u001b[39m=\u001b[39;49myearfirst,\n\u001b[0;32m    442\u001b[0m     utc\u001b[39m=\u001b[39;49mutc,\n\u001b[0;32m    443\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    444\u001b[0m     require_iso8601\u001b[39m=\u001b[39;49mrequire_iso8601,\n\u001b[0;32m    445\u001b[0m     allow_object\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m )\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m tz_parsed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m     \u001b[39m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     \u001b[39m# is in UTC\u001b[39;00m\n\u001b[0;32m    451\u001b[0m     dta \u001b[39m=\u001b[39m DatetimeArray(result, dtype\u001b[39m=\u001b[39mtz_to_dtype(tz_parsed))\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2177\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2175\u001b[0m order: Literal[\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m flags\u001b[39m.\u001b[39mf_contiguous \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2177\u001b[0m     result, tz_parsed \u001b[39m=\u001b[39m tslib\u001b[39m.\u001b[39;49marray_to_datetime(\n\u001b[0;32m   2178\u001b[0m         data\u001b[39m.\u001b[39;49mravel(\u001b[39m\"\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   2179\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   2180\u001b[0m         utc\u001b[39m=\u001b[39;49mutc,\n\u001b[0;32m   2181\u001b[0m         dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[0;32m   2182\u001b[0m         yearfirst\u001b[39m=\u001b[39;49myearfirst,\n\u001b[0;32m   2183\u001b[0m         require_iso8601\u001b[39m=\u001b[39;49mrequire_iso8601,\n\u001b[0;32m   2184\u001b[0m         allow_mixed\u001b[39m=\u001b[39;49mallow_mixed,\n\u001b[0;32m   2185\u001b[0m     )\n\u001b[0;32m   2186\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreshape(data\u001b[39m.\u001b[39mshape, order\u001b[39m=\u001b[39morder)\n\u001b[0;32m   2187\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOverflowError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   2188\u001b[0m     \u001b[39m# Exception is raised when a part of date is greater than 32 bit signed int\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx:427\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx:605\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\parsing.pyx:318\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1368\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[39mreturn\u001b[39;00m parser(parserinfo)\u001b[39m.\u001b[39mparse(timestr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     \u001b[39mreturn\u001b[39;00m DEFAULTPARSER\u001b[39m.\u001b[39mparse(timestr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:640\u001b[0m, in \u001b[0;36mparser.parse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m     default \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mreplace(hour\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, minute\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m    638\u001b[0m                                               second\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, microsecond\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 640\u001b[0m res, skipped_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse(timestr, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    642\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m ParserError(\u001b[39m\"\u001b[39m\u001b[39mUnknown string format: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, timestr)\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:856\u001b[0m, in \u001b[0;36mparser._parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    853\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    855\u001b[0m \u001b[39m# Process year/month/day\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m year, month, day \u001b[39m=\u001b[39m ymd\u001b[39m.\u001b[39;49mresolve_ymd(yearfirst, dayfirst)\n\u001b[0;32m    858\u001b[0m res\u001b[39m.\u001b[39mcentury_specified \u001b[39m=\u001b[39m ymd\u001b[39m.\u001b[39mcentury_specified\n\u001b[0;32m    859\u001b[0m res\u001b[39m.\u001b[39myear \u001b[39m=\u001b[39m year\n",
      "File \u001b[1;32mc:\\Users\\Silvia\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:484\u001b[0m, in \u001b[0;36m_ymd.resolve_ymd\u001b[1;34m(self, yearfirst, dayfirst)\u001b[0m\n\u001b[0;32m    478\u001b[0m strids \u001b[39m=\u001b[39m ((\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mystridx),\n\u001b[0;32m    479\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmstridx),\n\u001b[0;32m    480\u001b[0m           (\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdstridx))\n\u001b[0;32m    482\u001b[0m strids \u001b[39m=\u001b[39m {key: val \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m strids \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m    483\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(strids) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m         (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39;49m(strids) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m)):\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_from_stridxs(strids)\n\u001b[0;32m    487\u001b[0m mstridx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmstridx\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def get_sample_taxi_data() -> pd.core.frame.DataFrame:\n",
    "    sample_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    # do the sampling of taxi data according to the number of uber trip each month\n",
    "    for i in range(0, 78): \n",
    "        n = number[i]\n",
    "        url = all_parquet_urls[i]\n",
    "        df = get_and_clean_month_taxi_data(url)\n",
    "        df_sample = df.sample(n)\n",
    "        sample_taxi_dataframes.append(df_sample)\n",
    "\n",
    "    taxi_data = pd.concat(sample_taxi_dataframes)\n",
    "    taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    taxi_data = taxi_data.reset_index(drop = True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    # read file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    #drop unnecessary colums\n",
    "    df.drop(df.columns.difference(['DATE',\n",
    "                                   'HourlyPrecipitation', \n",
    "                                   'HourlyWindSpeed']), 1, inplace=True)\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.0)\n",
    "    # drop na values\n",
    "    df.dropna(subset=['HourlyWindSpeed'], inplace=True)\n",
    "    # convert \"DATE\" to datetime type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    # convert \"HourlyPrecipitation\" to float type\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    # fill in missing values\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    # cast \"df\" to specified type\n",
    "    df = df.astype({'HourlyWindSpeed': 'float32', 'HourlyPrecipitation': 'float32'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    # read file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Replace data of the string type\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.0)\n",
    "    # convert \"DATE\" to datetime type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    # convert \"HourlyPrecipitation\" to numeric type\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    # convert value of 'na' into 0.0\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "    #drop unnecessary colums\n",
    "    df.drop(df.columns.difference(['DATE',\n",
    "                                   'HourlyPrecipitation', \n",
    "                                   'HourlyWindSpeed']), 1, inplace=True)\n",
    "    # calculate hourly average as a daily values\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df = df.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': np.mean, 'HourlyPrecipitation': np.mean})\n",
    "    df['HourlyWindSpeed'] = df['HourlyWindSpeed'].map(lambda x: round(x, 2))\n",
    "    # remame columns\n",
    "    df.rename(columns={'HourlyWindSpeed': 'DailyAverageWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    df = df.astype({'DailyAverageWindSpeed':'float32', 'DailyPrecipitation':'float32', 'DATE' : 'datetime64[ns]'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sunset_sunrise_daily(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
    "    df = df.dropna()\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.astype({'Sunrise': 'int32', 'Sunset': 'int32', 'DATE':'datetime64[ns]' })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> pd.core.frame.DataFrame:\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = [\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2009_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2010_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2011_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2012_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2013_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2014_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\4501FinalProject_Group14\\2015_weather.csv\"\n",
    "        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_sunrise_sunset_data() -> pd.core.frame.DataFrame:\n",
    "    sunrise_sunset_dataframes =[]\n",
    "    \n",
    "    weather_csv_files = [\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2009_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2010_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2011_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2012_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2013_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2014_weather.csv\",\n",
    "            r\"C:\\Users\\Silvia\\Documents\\GitHub\\Projects-portfolio\\Uber_and_Yellow_Taxi\\2015_weather.csv\"\n",
    "        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        sunrise_sunset_dataframe = clean_sunset_sunrise_daily(csv_file)\n",
    "        sunrise_sunset_dataframes.append(sunrise_sunset_dataframe)\n",
    "        \n",
    "    sunrise_sunset_data = pd.concat(sunrise_sunset_dataframes)\n",
    "    sunrise_sunset_data['DATE'] = pd.to_datetime(sunrise_sunset_data['DATE'])\n",
    "    sunrise_sunset_data = sunrise_sunset_data.astype({'Sunrise': 'int32', 'Sunset': 'int32'})\n",
    "    \n",
    "    return sunrise_sunset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2099839999.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:3: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\2252719857.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE',\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:2: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "C:\\Users\\Silvia\\AppData\\Local\\Temp\\ipykernel_6592\\4001209316.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df.drop(df.columns.difference(['DATE','Sunset','Sunrise']), 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()\n",
    "sunrise_sunset_data = load_and_clean_sunrise_sunset_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
